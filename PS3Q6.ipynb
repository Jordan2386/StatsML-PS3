{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainx = pd.read_csv(\"usps_trainx.data\", header=None, delimiter=r\"\\s+\")\n",
    "trainy = pd.read_csv(\"usps_trainy.data\", header=None, delimiter=r\"\\s+\")\n",
    "testx = pd.read_csv(\"usps_testx.data\", header=None, delimiter=r\"\\s+\")\n",
    "testy = pd.read_csv(\"usps_testy.data\", header=None, delimiter=r\"\\s+\")\n",
    "\n",
    "im_height = 16 #image height\n",
    "im_width = 16 #image width\n",
    "num_classes = 10 #number of classes for classificaiton\n",
    "\n",
    "#normalise the images\n",
    "trainx = trainx / 256\n",
    "testx  = testx  / 256\n",
    "\n",
    "#transform pandas to numpy arrays\n",
    "trainx = trainx.as_matrix()\n",
    "trainy = trainy.as_matrix()\n",
    "testx = testx.as_matrix()\n",
    "testy = testy.as_matrix()\n",
    "\n",
    "#remove the second dimensions:\n",
    "trainy = trainy[:,0]\n",
    "testy = testy[:,0]\n",
    "\n",
    "#randomly shuffle the train data\n",
    "import random\n",
    "random_idx = random.sample([x for x in range(len(trainx))],len(trainx))\n",
    "trainx = trainx[random_idx,]\n",
    "trainy = trainy[random_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Transform the images in a 16x16 form; the resulting tensor would be 2000x16x16 both for training and for test data.\n",
    "# Simplified by Dr Seth Flaxman\n",
    "trainx = trainx.reshape(len(trainx),16,16)\n",
    "testx = testx.reshape(len(trainx),16,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1-hot encode the labels trainy and testy\n",
    "# Simplified by Dr Seth Flaxman\n",
    "trainy = pd.get_dummies(trainy).as_matrix()\n",
    "testy = pd.get_dummies(testy).as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some notation for convenience\n",
    "train_size = len(trainx)\n",
    "test_size = len(testx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract batches funciton that returns a dictionary ready to be fed to the model\n",
    "\n",
    "def next_batch(data_x,data_y,indices):\n",
    "    return data_x[indices,:,:] , data_y[indices,:]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining the convolution operation (with relu activation), and the maxpool operation\n",
    "# https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3_NeuralNetworks/convolutional_network.ipynb\n",
    "\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    # MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n",
    "                          padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"   \\n    # Feed Dict and RUN SESSION\\n    num_batches = int(len(trainx)/batch_size)\\n\\n    sess = tf.Session()\\n    tf.global_variables_initializer().run(session=sess)\\n    # Feed the batches one by one\\n    for t in range(num_epochs):\\n        for i in range(num_batches):\\n            batch_X, batch_y = next_batch(trainx,trainy,[x for x in range(len(trainx))][i*batch_size:(i+1)*batch_size])\\n            sess.run(optimizer, feed_dict={X: batch_X, y: batch_y})\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CONSTRUCT THE COMPUTATIONAL GRAPH\n",
    "\n",
    "\n",
    "batch_size = 5\n",
    "num_epochs = 4  #optimal: 400\n",
    "learning_rate = 0.001\n",
    "\n",
    "c = 5 #convolution window size\n",
    "stride = 2 #stride of the convolution\n",
    "conv1_size = 64 #number of outputs from the conv layer\n",
    "\n",
    "c2 = 5 #2nd convolution window size\n",
    "stride_2 = 2 #stride of the 2nd convolution\n",
    "conv2_size = 32 #number of outputs from the 2nd conv layer\n",
    "\n",
    "full_conn_size = 1024 #number of nodes in the fully connedted layer\n",
    "\n",
    "compute_graph = tf.Graph()\n",
    "# CONSTRUCT THE GRAPH\n",
    "with compute_graph.as_default():\n",
    "    \n",
    "    #introduce placeholders for the data(images and labels) to later feed batches into\n",
    "    X = tf.placeholder(tf.float32, shape=(batch_size,im_height,im_width)) #images\n",
    "    y = tf.placeholder(tf.int32, shape=(batch_size,num_classes)) #labels\n",
    "    \n",
    "    #transform input into a 4d tensor to include the colour channels\n",
    "    X_prime = tf.reshape(X, shape=[-1, im_height, im_width, 1])\n",
    "    \n",
    "    #define the parameters(variables) of the model\n",
    "    #for the convolution layer:\n",
    "    W1 = tf.Variable(tf.random_normal([c, c, 1, conv1_size]))\n",
    "    b1 = tf.Variable(tf.random_normal([conv1_size]))\n",
    "    #for the second convolution layer:\n",
    "    W2 = tf.Variable(tf.random_normal([c2, c2, conv1_size, conv2_size]))\n",
    "    b2 = tf.Variable(tf.random_normal([conv2_size]))\n",
    "    #for the fully connected layer:\n",
    "    W3 = tf.Variable(tf.random_normal([1*1*conv2_size, full_conn_size]))\n",
    "    b3 = tf.Variable(tf.random_normal([full_conn_size]))\n",
    "    #for the output:\n",
    "    WO = tf.Variable(tf.random_normal([full_conn_size, num_classes]))\n",
    "    bO = tf.Variable(tf.random_normal([num_classes]))\n",
    "    \n",
    "    \n",
    "    #define the model structure\n",
    "    # c x c convolution, 1 input, conv1_size outputs\n",
    "    convolution = conv2d(X_prime, W1, b1, stride)\n",
    "    # then apply pooling\n",
    "    pool = maxpool2d(convolution)\n",
    "    \n",
    "    #one more convolution+pooling layer:\n",
    "    convolution2 = conv2d(pool, W2, b2, stride_2)\n",
    "    # then apply pooling\n",
    "    pool2 = maxpool2d(convolution2)\n",
    "    \n",
    "    \n",
    "    #add a fully connected layer\n",
    "    # Reshape convolution output to fit the fully connected layer input\n",
    "    full_conn = tf.reshape(pool2, [-1, 1*1*conv2_size])\n",
    "    full_conn = tf.add(tf.matmul(full_conn, W3), b3)\n",
    "    full_conn = tf.nn.relu(full_conn)\n",
    "    \n",
    "\n",
    "    # Output, class prediction\n",
    "    predictions = tf.add(tf.matmul(full_conn, WO), bO)\n",
    "    \n",
    "    # Define loss and optimizer\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=predictions, labels=y))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    \n",
    "    # Evaluate model\n",
    "    # https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3_NeuralNetworks/convolutional_network.ipynb\n",
    "    correct_pred = tf.equal(tf.argmax(predictions, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    \n",
    "\"\"\"\"   \n",
    "    # Feed Dict and RUN SESSION\n",
    "    num_batches = int(len(trainx)/batch_size)\n",
    "\n",
    "    sess = tf.Session()\n",
    "    tf.global_variables_initializer().run(session=sess)\n",
    "    # Feed the batches one by one\n",
    "    for t in range(num_epochs):\n",
    "        for i in range(num_batches):\n",
    "            batch_X, batch_y = next_batch(trainx,trainy,[x for x in range(len(trainx))][i*batch_size:(i+1)*batch_size])\n",
    "            sess.run(optimizer, feed_dict={X: batch_X, y: batch_y})\n",
    "\"\"\"\n",
    "\n",
    "#    for _ in range(1000):\n",
    "#        batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "#        sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-21-9769c9b164bb>:23 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Train accuracy at epoch 1 is: 0.363888896546\n",
      "Valid accuracy at epoch 1 is: 0.575000014156\n",
      "Train accuracy at epoch 2 is: 0.67555556794\n",
      "Valid accuracy at epoch 2 is: 0.685000012815\n",
      "Train accuracy at epoch 3 is: 0.780555566276\n",
      "Valid accuracy at epoch 3 is: 0.810000009835\n",
      "Train accuracy at epoch 4 is: 0.843888897821\n",
      "Valid accuracy at epoch 4 is: 0.820000009239\n"
     ]
    }
   ],
   "source": [
    "# TUNE HYPERPARAMETERS\n",
    "# Note: this can take a while to run\n",
    "# Extract (from the test set) a small validation set to test the model accuracy while tuning the hyperparameters\n",
    "\n",
    "valid_size = 200\n",
    "\n",
    "trainx_ = trainx[0:train_size-valid_size]\n",
    "trainy_ = trainy[0:train_size-valid_size]\n",
    "validx = trainx[train_size-valid_size:]\n",
    "validy = trainy[train_size-valid_size:]\n",
    "\n",
    "num_train_batches = int(len(trainx_)/batch_size)\n",
    "train_accuracy = np.zeros(num_train_batches)\n",
    "\n",
    "num_valid_batches = int(len(validx)/batch_size)\n",
    "valid_accuracy = np.zeros(num_valid_batches)\n",
    "\n",
    "\n",
    "# Initialise all variables and run the session:\n",
    "with tf.Session(graph=compute_graph) as sess:\n",
    "    tf.initialize_all_variables().run(session=sess)\n",
    "    \n",
    "    # Feed the batches one by one\n",
    "    for t in range(num_epochs):\n",
    "        for i in range(num_train_batches):\n",
    "            batch_X, batch_y = next_batch(trainx_,trainy_,[x for x in range(train_size-valid_size)][i*batch_size:(i+1)*batch_size])\n",
    "            sess.run(optimizer, feed_dict={X: batch_X, y: batch_y})\n",
    "            train_accuracy[i] = sess.run(accuracy, feed_dict={X: batch_X, y: batch_y})\n",
    "        \n",
    "        # report train/valid accuracy regularly\n",
    "        if(t % 100 == 0 or (t % 10 == 0 and t <= 100) or t <= 10):\n",
    "            print(\"Train accuracy at epoch\", t+1, \"is:\", sum(train_accuracy)/num_train_batches)\n",
    "            # validation accuracy\n",
    "            for i in range(num_valid_batches):\n",
    "                batch_X, batch_y = next_batch(validx,validy,[x for x in range(train_size-valid_size)][i*batch_size:(i+1)*batch_size])\n",
    "                sess.run(accuracy, feed_dict={X: batch_X, y: batch_y})\n",
    "                valid_accuracy[i] = sess.run(accuracy, feed_dict={X: batch_X, y: batch_y})\n",
    "            print(\"Valid accuracy at epoch\", t+1, \"is:\", sum(valid_accuracy)/num_valid_batches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TUNE HYPERPARAMETERS\n",
    "# Do cross-validation to check the model performance and tune the parameters (num layers, layer types, batch size, etc.)\n",
    "# Note: it's not wise to use the test set in hyperparameter tuning\n",
    "\n",
    "num_cross_valid = 10 #number of cross-validation sectors\n",
    "train_accuracy = np.zeros(num_cross_valid)\n",
    "\n",
    "# Train the model on each 9 of the 10 train data sectors, while recording the test accuracy on the remaining segment\n",
    "for i in range(num_cross_valid):\n",
    "    train_batch_X, train_batch_y = next_batch(trainx,trainy,[x for x in range(len(trainx))][i*batch_size:(i+1)*batch_size])\n",
    "    sess.run(optimizer, feed_dict={X: train_batch_X, y: train_batch_y})\n",
    "    test_batch_X, test_batch_y = next_batch(trainx,trainy,[x for x in range(len(trainx))][i*batch_size:(i+1)*batch_size])\n",
    "    train_accuracy[i] = sess.run(accuracy, feed_dict={X: batch_X, y: batch_y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOW TRAIN THE MODEL OVER THE WHOLE DATASET\n",
    "\n",
    "num_batches = int(len(trainx)/batch_size)\n",
    "train_accuracy = np.zeros(num_batches)\n",
    "\n",
    "# Initialise all variables and run the session\n",
    "sess = tf.Session()\n",
    "tf.global_variables_initializer().run(session=sess)\n",
    "\n",
    "# Feed the batches one by one\n",
    "for t in range(num_epochs):\n",
    "    for i in range(num_batches):\n",
    "        batch_X, batch_y = next_batch(trainx,trainy,[x for x in range(len(trainx))][i*batch_size:(i+1)*batch_size])\n",
    "        sess.run(optimizer, feed_dict={X: batch_X, y: batch_y})\n",
    "        train_accuracy[i] = sess.run(accuracy, feed_dict={X: batch_X, y: batch_y})\n",
    "    print(\"The train accuracy at epoch\", t, \"is:\", sum(train_accuracy)/num_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test accuracy is  0.944500003159\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the algorithm. Note: This is not elegant since I couldn't find a way to introduce a tensor of size depending on other tensor in the Graph above\n",
    "# Note: the train set is used as a validation set, thus one can consider finding a different test set, or using cross-validation\n",
    "\n",
    "num_test_batches = int(len(testx)/batch_size)\n",
    "accuracy_ = np.zeros(num_test_batches)\n",
    "\n",
    "for i in range(num_test_batches):\n",
    "        batch_X, batch_y = next_batch(testx,testy,[x for x in range(len(testx))][i*batch_size:(i+1)*batch_size])\n",
    "        accuracy_[i] = sess.run(accuracy, feed_dict={X: batch_X, y: batch_y})\n",
    "        \n",
    "print(\"The test accuracy is \",sum(accuracy_)/num_test_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I should have developed some tools to output the train accuracy at each epoch, but I'll call it a day"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
