{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainx = pd.read_csv(\"usps_trainx.data\", header=None, delimiter=r\"\\s+\")\n",
    "trainy = pd.read_csv(\"usps_trainy.data\", header=None, delimiter=r\"\\s+\")\n",
    "testx = pd.read_csv(\"usps_testx.data\", header=None, delimiter=r\"\\s+\")\n",
    "testy = pd.read_csv(\"usps_testy.data\", header=None, delimiter=r\"\\s+\")\n",
    "\n",
    "im_height = 16 #image height\n",
    "im_width = 16 #image width\n",
    "num_classes = 10 #number of classes for classificaiton\n",
    "\n",
    "#normalise the images\n",
    "trainx = trainx / 256\n",
    "testx  = testx  / 256\n",
    "\n",
    "#transform pandas to numpy arrays\n",
    "trainx = trainx.as_matrix()\n",
    "trainy = trainy.as_matrix()\n",
    "testx = testx.as_matrix()\n",
    "testy = testy.as_matrix()\n",
    "\n",
    "#remove the second dimensions:\n",
    "trainy = trainy[:,0]\n",
    "testy = testy[:,0]\n",
    "\n",
    "#randomly shuffle the train data\n",
    "import random\n",
    "random_idx = random.sample([x for x in range(len(trainx))],len(trainx))\n",
    "trainx = trainx[random_idx,]\n",
    "trainy = trainy[random_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Transform the images in a 16x16 form; the resulting tensor would be 2000x16x16 both for training and for test data.\n",
    "# Simplified by Dr Seth Flaxman\n",
    "trainx = trainx.reshape(len(trainx),16,16)\n",
    "testx = testx.reshape(len(trainx),16,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1-hot encode the labels trainy and testy\n",
    "# Simplified by Dr Seth Flaxman\n",
    "trainy = pd.get_dummies(trainy).as_matrix()\n",
    "testy = pd.get_dummies(testy).as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some notation for convenience\n",
    "train_size = len(trainx)\n",
    "test_size = len(testx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract batches funciton that returns a dictionary ready to be fed to the model\n",
    "\n",
    "def next_batch(data_x,data_y,indices):\n",
    "    return data_x[indices,:,:] , data_y[indices,:]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining the convolution operation (with relu activation), and the maxpool operation\n",
    "# https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3_NeuralNetworks/convolutional_network.ipynb\n",
    "\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def maxpool2d(x, k=2, s=2):\n",
    "    # MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, s, s, 1],\n",
    "                          padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"   \\n    # Feed Dict and RUN SESSION\\n    num_batches = int(len(trainx)/batch_size)\\n\\n    sess = tf.Session()\\n    tf.global_variables_initializer().run(session=sess)\\n    # Feed the batches one by one\\n    for t in range(num_epochs):\\n        for i in range(num_batches):\\n            batch_X, batch_y = next_batch(trainx,trainy,[x for x in range(len(trainx))][i*batch_size:(i+1)*batch_size])\\n            sess.run(optimizer, feed_dict={X: batch_X, y: batch_y})\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CONSTRUCT THE COMPUTATIONAL GRAPH\n",
    "\n",
    "\n",
    "# Introduce some hyperparameters\n",
    "\n",
    "batch_size = 20\n",
    "# an epoch is a single run over all the training data\n",
    "num_epochs = 200 #optimal: 400\n",
    "learning_rate = 0.001\n",
    "# applying dropout makes sure that the model doesn't rely on any specific parameter, and is thus more robust and prevents overfitting\n",
    "dropout = 0.98 # probability to keep a parameter\n",
    "# l2 - regularisation parameter\n",
    "alpha = 1\n",
    "\n",
    "c = 5 #convolution window size\n",
    "stride = 2 #stride of the convolution #DON'T CHANGE\n",
    "conv1_size = 64 #number of outputs from the conv layer\n",
    "\n",
    "c2 = 5 #2nd convolution window size\n",
    "stride_2 = 2 #stride of the 2nd convolution #DON'T CHANGE\n",
    "conv2_size = 32 #number of outputs from the 2nd conv layer\n",
    "\n",
    "c3= 5 #3rd convolution window size\n",
    "stride_3 = 2 #stride of the 2nd convolution #DON'T CHANGE\n",
    "conv3_size = 128 #number of outputs from the 2nd conv layer\n",
    "\n",
    "full_conn_size = 1024 #number of nodes in the fully connected layer\n",
    "\n",
    "\n",
    "# Construct the graph\n",
    "\n",
    "compute_graph = tf.Graph()\n",
    "with compute_graph.as_default():\n",
    "    \n",
    "    #introduce placeholders for the data(images and labels) to later feed batches into\n",
    "    X = tf.placeholder(tf.float32, shape=(batch_size,im_height,im_width)) #images\n",
    "    y = tf.placeholder(tf.int32, shape=(batch_size,num_classes)) #labels\n",
    "\n",
    "    \n",
    "    #transform input into a 4d tensor to include the colour channels (TensorFlow demands that)\n",
    "    X_prime = tf.reshape(X, shape=[-1, im_height, im_width, 1])\n",
    "    \n",
    "    #define the parameters(variables) of the model\n",
    "    #for the convolution layer:\n",
    "    W1 = tf.Variable(tf.random_normal([c, c, 1, conv1_size]))\n",
    "    b1 = tf.Variable(tf.random_normal([conv1_size]))\n",
    "    #for the second convolution layer:\n",
    "    W2 = tf.Variable(tf.random_normal([c2, c2, conv1_size, conv2_size]))\n",
    "    b2 = tf.Variable(tf.random_normal([conv2_size]))\n",
    "    #for the third convolution layer\n",
    "    W3 = tf.Variable(tf.random_normal([c3, c3, conv2_size, conv3_size]))\n",
    "    b3 = tf.Variable(tf.random_normal([conv3_size]))\n",
    "    #for the fully connected layer:\n",
    "    W4 = tf.Variable(tf.random_normal([2*2*conv3_size, full_conn_size]))\n",
    "    b4 = tf.Variable(tf.random_normal([full_conn_size]))\n",
    "    #for the output:\n",
    "    WO = tf.Variable(tf.random_normal([full_conn_size, num_classes]))\n",
    "    bO = tf.Variable(tf.random_normal([num_classes]))\n",
    "    \n",
    "    \n",
    "    #define the model structure\n",
    "    # c x c convolution, 1 input, conv1_size outputs\n",
    "    convolution = conv2d(X_prime, W1, b1, stride)\n",
    "    # then apply pooling\n",
    "    pool = maxpool2d(convolution,2,1)\n",
    "    \n",
    "    #one more convolution+pooling layer:\n",
    "    convolution2 = conv2d(pool, W2, b2, stride_2)\n",
    "    # then apply pooling\n",
    "    pool2 = maxpool2d(convolution2,2,1)\n",
    "    \n",
    "    #one more convolution+pooling layer:\n",
    "    convolution3 = conv2d(pool2, W3, b3, stride_3)\n",
    "    # then apply pooling\n",
    "    pool3 = maxpool2d(convolution3,2,1)\n",
    "    \n",
    "    \n",
    "    #add a fully connected layer\n",
    "    # Reshape convolution output to fit the fully connected layer input\n",
    "    full_conn = tf.reshape(pool3, [-1, 2*2*conv3_size])\n",
    "    full_conn = tf.add(tf.matmul(full_conn, W4), b4)\n",
    "    full_conn = tf.nn.relu(full_conn)\n",
    "    # The dropout is applied (almost) at the end of the neural network\n",
    "    full_conn = tf.nn.dropout(full_conn, dropout)\n",
    "\n",
    "    # Output, class prediction\n",
    "    predictions = tf.add(tf.matmul(full_conn, WO), bO)\n",
    "    \n",
    "    # Define loss and optimizer\n",
    "    # To the cost add the l2_loss of all the weights (but not the biases)\n",
    "    cost = (tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=predictions, labels=y)) + \\\n",
    "            alpha * (tf.nn.l2_loss(WO) + tf.nn.l2_loss(W1) + tf.nn.l2_loss(W2) + tf.nn.l2_loss(W3)))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    \n",
    "    # Evaluate model\n",
    "    # https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3_NeuralNetworks/convolutional_network.ipynb\n",
    "    correct_pred = tf.equal(tf.argmax(predictions, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    \n",
    "\"\"\"\"   \n",
    "    # Feed Dict and RUN SESSION\n",
    "    num_batches = int(len(trainx)/batch_size)\n",
    "\n",
    "    sess = tf.Session()\n",
    "    tf.global_variables_initializer().run(session=sess)\n",
    "    # Feed the batches one by one\n",
    "    for t in range(num_epochs):\n",
    "        for i in range(num_batches):\n",
    "            batch_X, batch_y = next_batch(trainx,trainy,[x for x in range(len(trainx))][i*batch_size:(i+1)*batch_size])\n",
    "            sess.run(optimizer, feed_dict={X: batch_X, y: batch_y})\n",
    "\"\"\"\n",
    "\n",
    "#    for _ in range(1000):\n",
    "#        batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "#        sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-12-1344162aa1ee>:21 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Train accuracy at epoch 1 is: 0.302777780675\n",
      "Valid accuracy at epoch 1 is: 0.434999999404\n",
      "Train accuracy at epoch 2 is: 0.58388888869\n",
      "Valid accuracy at epoch 2 is: 0.565000003576\n",
      "Train accuracy at epoch 3 is: 0.693333334062\n",
      "Valid accuracy at epoch 3 is: 0.695000010729\n",
      "Train accuracy at epoch 4 is: 0.77222222156\n",
      "Valid accuracy at epoch 4 is: 0.764999997616\n",
      "Train accuracy at epoch 5 is: 0.825000001987\n",
      "Valid accuracy at epoch 5 is: 0.744999992847\n",
      "Train accuracy at epoch 6 is: 0.833333333333\n",
      "Valid accuracy at epoch 6 is: 0.780000001192\n",
      "Train accuracy at epoch 7 is: 0.861111109787\n",
      "Valid accuracy at epoch 7 is: 0.830000001192\n",
      "Train accuracy at epoch 8 is: 0.893888882134\n",
      "Valid accuracy at epoch 8 is: 0.785000008345\n",
      "Train accuracy at epoch 9 is: 0.905555549595\n",
      "Valid accuracy at epoch 9 is: 0.8\n",
      "Train accuracy at epoch 10 is: 0.915555550655\n",
      "Valid accuracy at epoch 10 is: 0.790000003576\n",
      "Train accuracy at epoch 15 is: 0.964999993642\n",
      "Valid accuracy at epoch 15 is: 0.855000007153\n",
      "Train accuracy at epoch 20 is: 0.989999997616\n",
      "Valid accuracy at epoch 20 is: 0.884999996424\n",
      "Train accuracy at epoch 25 is: 0.98888888624\n",
      "Valid accuracy at epoch 25 is: 0.884999990463\n",
      "Train accuracy at epoch 30 is: 0.988333330552\n",
      "Valid accuracy at epoch 30 is: 0.860000014305\n",
      "Train accuracy at epoch 35 is: 0.986666663488\n",
      "Valid accuracy at epoch 35 is: 0.880000001192\n",
      "Train accuracy at epoch 40 is: 0.989999997616\n",
      "Valid accuracy at epoch 40 is: 0.870000004768\n",
      "Train accuracy at epoch 45 is: 0.993888887432\n",
      "Valid accuracy at epoch 45 is: 0.880000001192\n",
      "Train accuracy at epoch 50 is: 0.985555552774\n",
      "Valid accuracy at epoch 50 is: 0.879999995232\n",
      "Train accuracy at epoch 55 is: 0.993333331744\n",
      "Valid accuracy at epoch 55 is: 0.884999996424\n",
      "Train accuracy at epoch 60 is: 0.994999998808\n",
      "Valid accuracy at epoch 60 is: 0.904999995232\n",
      "Train accuracy at epoch 65 is: 0.993888887432\n",
      "Valid accuracy at epoch 65 is: 0.875\n",
      "Train accuracy at epoch 70 is: 0.99166666468\n",
      "Valid accuracy at epoch 70 is: 0.904999995232\n",
      "Train accuracy at epoch 75 is: 0.993333331744\n",
      "Valid accuracy at epoch 75 is: 0.92499999404\n",
      "Train accuracy at epoch 80 is: 0.999444444312\n",
      "Valid accuracy at epoch 80 is: 0.910000002384\n",
      "Train accuracy at epoch 85 is: 0.996666665872\n",
      "Valid accuracy at epoch 85 is: 0.930000001192\n",
      "Train accuracy at epoch 90 is: 0.99722222156\n",
      "Valid accuracy at epoch 90 is: 0.909999990463\n",
      "Train accuracy at epoch 95 is: 0.993333331744\n",
      "Valid accuracy at epoch 95 is: 0.904999989271\n",
      "Train accuracy at epoch 100 is: 0.992222220368\n",
      "Valid accuracy at epoch 100 is: 0.910000008345\n",
      "Train accuracy at epoch 110 is: 0.997777777248\n",
      "Valid accuracy at epoch 110 is: 0.929999995232\n",
      "Train accuracy at epoch 120 is: 1.0\n",
      "Valid accuracy at epoch 120 is: 0.929999989271\n",
      "Train accuracy at epoch 130 is: 0.993333331744\n",
      "Valid accuracy at epoch 130 is: 0.944999986887\n",
      "Train accuracy at epoch 140 is: 0.996111110184\n",
      "Valid accuracy at epoch 140 is: 0.919999998808\n",
      "Train accuracy at epoch 150 is: 0.998888888624\n",
      "Valid accuracy at epoch 150 is: 0.934999996424\n",
      "Train accuracy at epoch 160 is: 0.999444444312\n",
      "Valid accuracy at epoch 160 is: 0.929999989271\n",
      "Train accuracy at epoch 170 is: 0.998888888624\n",
      "Valid accuracy at epoch 170 is: 0.935000002384\n",
      "Train accuracy at epoch 180 is: 0.995555554496\n",
      "Valid accuracy at epoch 180 is: 0.954999989271\n",
      "Train accuracy at epoch 190 is: 0.999444444312\n",
      "Valid accuracy at epoch 190 is: 0.959999996424\n",
      "Train accuracy at epoch 200 is: 0.997777777248\n",
      "Valid accuracy at epoch 200 is: 0.94999999404\n"
     ]
    }
   ],
   "source": [
    "# TUNE HYPERPARAMETERS\n",
    "# Note: this can take a while to run\n",
    "# Extract (from the test set) a small validation set to test the model accuracy while tuning the hyperparameters\n",
    "\n",
    "valid_size = 200\n",
    "\n",
    "trainx_ = trainx[0:train_size-valid_size]\n",
    "trainy_ = trainy[0:train_size-valid_size]\n",
    "validx = trainx[train_size-valid_size:]\n",
    "validy = trainy[train_size-valid_size:]\n",
    "\n",
    "num_train_batches = int(len(trainx_)/batch_size)\n",
    "train_accuracy = np.zeros(num_train_batches)\n",
    "\n",
    "num_valid_batches = int(len(validx)/batch_size)\n",
    "valid_accuracy = np.zeros(num_valid_batches)\n",
    "\n",
    "\n",
    "# Initialise all variables and run the session:\n",
    "with tf.Session(graph=compute_graph) as sess:\n",
    "    tf.initialize_all_variables().run(session=sess)\n",
    "    \n",
    "    # Feed the batches one by one\n",
    "    for t in range(num_epochs):\n",
    "        for i in range(num_train_batches):\n",
    "            batch_X, batch_y = next_batch(trainx_,trainy_,[x for x in range(train_size-valid_size)][i*batch_size:(i+1)*batch_size])\n",
    "            sess.run(optimizer, feed_dict={X: batch_X, y: batch_y})\n",
    "            train_accuracy[i] = sess.run(accuracy, feed_dict={X: batch_X, y: batch_y})\n",
    "        \n",
    "        # report train/valid accuracy regularly in order to perform early stoping\n",
    "        if((t+1) % 10 == 0 or ((t+1) % 5 == 0 and (t+1) <= 100) or (t+1) <= 10):\n",
    "            print(\"Train accuracy at epoch\", t+1, \"is:\", sum(train_accuracy)/num_train_batches)\n",
    "            # validation accuracy\n",
    "            for i in range(num_valid_batches):\n",
    "                batch_X, batch_y = next_batch(validx,validy,[x for x in range(valid_size)][i*batch_size:(i+1)*batch_size])\n",
    "                valid_accuracy[i] = sess.run(accuracy, feed_dict={X: batch_X, y: batch_y})\n",
    "            print(\"Valid accuracy at epoch\", t+1, \"is:\", sum(valid_accuracy)/num_valid_batches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TUNE HYPERPARAMETERS\n",
    "# Do cross-validation to check the model performance and tune the parameters (num layers, layer types, batch size, etc.)\n",
    "# Note: it's not wise to use the test set in hyperparameter tuning\n",
    "# Note: cross-validation is much slower than fixed-set validation\n",
    "\n",
    "#num_cross_valid = 10 #number of cross-validation sectors\n",
    "#train_accuracy = np.zeros(num_cross_valid)\n",
    "\n",
    "# Train the model on each 9 of the 10 train data sectors, while recording the test accuracy on the remaining segment\n",
    "#for i in range(num_cross_valid):\n",
    "#    train_batch_X, train_batch_y = next_batch(trainx,trainy,[x for x in range(len(trainx))][i*batch_size:(i+1)*batch_size])\n",
    "#    sess.run(optimizer, feed_dict={X: train_batch_X, y: train_batch_y})\n",
    "#    test_batch_X, test_batch_y = next_batch(trainx,trainy,[x for x in range(len(trainx))][i*batch_size:(i+1)*batch_size])\n",
    "#    train_accuracy[i] = sess.run(accuracy, feed_dict={X: batch_X, y: batch_y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-13-267ffd6154d4>:12 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "The train accuracy at epoch 1 is: 0.359000002071\n",
      "The train accuracy at epoch 2 is: 0.635499999523\n",
      "The train accuracy at epoch 3 is: 0.731999997795\n",
      "The train accuracy at epoch 4 is: 0.793499997854\n",
      "The train accuracy at epoch 5 is: 0.82650000155\n",
      "The train accuracy at epoch 6 is: 0.867999999523\n",
      "The train accuracy at epoch 7 is: 0.889499999285\n",
      "The train accuracy at epoch 8 is: 0.918999990821\n",
      "The train accuracy at epoch 9 is: 0.934499993324\n",
      "The train accuracy at epoch 10 is: 0.941499991417\n",
      "The train accuracy at epoch 15 is: 0.971499994993\n",
      "The train accuracy at epoch 20 is: 0.982999995947\n",
      "The train accuracy at epoch 25 is: 0.984499996305\n",
      "The train accuracy at epoch 30 is: 0.992999998331\n",
      "The train accuracy at epoch 35 is: 0.989999997616\n",
      "The train accuracy at epoch 40 is: 0.990999997854\n",
      "The train accuracy at epoch 45 is: 0.989999997616\n",
      "The train accuracy at epoch 50 is: 0.989499998093\n",
      "The train accuracy at epoch 55 is: 0.996999999285\n",
      "The train accuracy at epoch 60 is: 0.991499997973\n",
      "The train accuracy at epoch 65 is: 0.993999998569\n",
      "The train accuracy at epoch 70 is: 0.996999999285\n",
      "The train accuracy at epoch 75 is: 0.993999998569\n",
      "The train accuracy at epoch 80 is: 0.996499999166\n",
      "The train accuracy at epoch 85 is: 0.998499999642\n",
      "The train accuracy at epoch 90 is: 0.994999998808\n",
      "The train accuracy at epoch 95 is: 0.997499999404\n",
      "The train accuracy at epoch 100 is: 0.997499999404\n",
      "The train accuracy at epoch 120 is: 0.998499999642\n",
      "The train accuracy at epoch 140 is: 0.999499999881\n",
      "The train accuracy at epoch 160 is: 0.998999999762\n",
      "The train accuracy at epoch 180 is: 0.999499999881\n",
      "The train accuracy at epoch 200 is: 0.993999998569\n",
      "The test accuracy is  0.933999994397\n"
     ]
    }
   ],
   "source": [
    "# NOW TRAIN THE MODEL OVER THE WHOLE DATASET AND REPORT TEST ACCURACY\n",
    "# Note: this can take a while to run\n",
    "\n",
    "num_batches = int(train_size/batch_size)\n",
    "train_accuracy = np.zeros(num_batches)\n",
    "\n",
    "num_test_batches = int(test_size/batch_size)\n",
    "accuracy_ = np.zeros(num_test_batches)\n",
    "\n",
    "# Initialise all variables and run the session\n",
    "with tf.Session(graph=compute_graph) as sess:\n",
    "    tf.initialize_all_variables().run(session=sess)\n",
    "\n",
    "    # Feed the batches one by one\n",
    "    for t in range(num_epochs):\n",
    "        for i in range(num_batches):\n",
    "            batch_X, batch_y = next_batch(trainx,trainy,[x for x in range(train_size)][i*batch_size:(i+1)*batch_size])\n",
    "            sess.run(optimizer, feed_dict={X: batch_X, y: batch_y})\n",
    "            train_accuracy[i] = sess.run(accuracy, feed_dict={X: batch_X, y: batch_y})\n",
    "        if((t+1) % 20 == 0 or ((t+1) % 5 == 0 and (t+1) <= 100) or (t+1) <= 10):\n",
    "            print(\"The train accuracy at epoch\", t+1, \"is:\", sum(train_accuracy)/num_batches)\n",
    "            \n",
    "    \n",
    "        \n",
    "    \n",
    "    for i in range(num_test_batches):\n",
    "        batch_X, batch_y = next_batch(testx,testy,[x for x in range(len(testx))][i*batch_size:(i+1)*batch_size])\n",
    "        accuracy_[i] = sess.run(accuracy, feed_dict={X: batch_X, y: batch_y})\n",
    "        \n",
    "    print(\"The test accuracy is \",sum(accuracy_)/num_test_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
