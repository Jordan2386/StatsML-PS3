{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainx = pd.read_csv(\"usps_trainx.data\", header=None, delimiter=r\"\\s+\")\n",
    "trainy = pd.read_csv(\"usps_trainy.data\", header=None, delimiter=r\"\\s+\")\n",
    "testx = pd.read_csv(\"usps_testx.data\", header=None, delimiter=r\"\\s+\")\n",
    "testy = pd.read_csv(\"usps_testy.data\", header=None, delimiter=r\"\\s+\")\n",
    "\n",
    "im_height = 16 #image height\n",
    "im_width = 16 #image width\n",
    "num_classes = 10 #number of classes for classificaiton\n",
    "\n",
    "#normalise the images\n",
    "trainx = trainx / 256\n",
    "testx  = testx  / 256\n",
    "\n",
    "#transform pandas to numpy arrays\n",
    "trainx = trainx.as_matrix()\n",
    "trainy = trainy.as_matrix()\n",
    "testx = testx.as_matrix()\n",
    "testy = testy.as_matrix()\n",
    "\n",
    "#remove the second dimensions:\n",
    "trainy = trainy[:,0]\n",
    "testy = testy[:,0]\n",
    "\n",
    "#randomly shuffle the train data\n",
    "import random\n",
    "random_idx = random.sample([x for x in range(len(trainx))],len(trainx))\n",
    "trainx = trainx[random_idx,]\n",
    "trainy = trainy[random_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Transform the images in a 16x16 form; the resulting tensor would be 2000x16x16 both for training and for test data.\n",
    "# Simplified by Dr Seth Flaxman\n",
    "trainx = trainx.reshape(len(trainx),16,16)\n",
    "testx = testx.reshape(len(trainx),16,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#1-hot encode the labels trainy and testy\n",
    "train_y = np.zeros((len(trainy),num_classes))\n",
    "test_y = np.zeros((len(testy),num_classes))\n",
    "def one_hot(labels,labels1):\n",
    "    for i in range(len(labels)):\n",
    "        for j in range(num_classes):\n",
    "            if labels[i] == j:\n",
    "                labels1[i,j] = 1\n",
    "            else:\n",
    "                labels1[i,j] = 0\n",
    "    return labels1\n",
    "\n",
    "trainy = one_hot(trainy, train_y)\n",
    "testy = one_hot(testy, test_y)\n",
    "train_y = None\n",
    "test_y = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Extract batches funciton that returns a dictionary ready to be fed to the model\n",
    "\n",
    "def next_batch(data_x,data_y,indices):\n",
    "    return data_x[indices,:,:] , data_y[indices,:]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining the convolution operation (with relu activation), and the maxpool operation\n",
    "# https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3_NeuralNetworks/convolutional_network.ipynb\n",
    "\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    # MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n",
    "                          padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#CONSTRUCT THE COMPUTATIONAL GRAPH\n",
    "\n",
    "\n",
    "batch_size = 5\n",
    "num_epochs = 400\n",
    "learning_rate = 0.001\n",
    "\n",
    "conv1_size = 64 #number of outputs from the conv layer\n",
    "c = 5 #convolution window size\n",
    "stride = 2 #stride of the convolution\n",
    "\n",
    "conv2_size = 32 #number of outputs from the 2nd conv layer\n",
    "c2 = 5 #2nd convolution window size\n",
    "stride_2 = 2 #stride of the 2nd convolution\n",
    "\n",
    "full_conn_size = 1024 #number of nodes in the fully connedted layer\n",
    "\n",
    "\n",
    "#CONSTRUCT THE GRAPH\n",
    "with tf.Graph().as_default():\n",
    "    \n",
    "    #introduce placeholders for the data(images and labels) to later feed batches into\n",
    "    X = tf.placeholder(tf.float32, shape=(batch_size,im_height,im_width)) #images\n",
    "    y = tf.placeholder(tf.int32, shape=(batch_size,num_classes)) #labels\n",
    "    \n",
    "    #transform input into a 4d tensor to include the colour channels\n",
    "    X_prime = tf.reshape(X, shape=[-1, im_height, im_width, 1])\n",
    "    \n",
    "    #define the parameters(variables) of the model\n",
    "    #for the convolution layer:\n",
    "    W1 = tf.Variable(tf.random_normal([c, c, 1, conv1_size]))\n",
    "    b1 = tf.Variable(tf.random_normal([conv1_size]))\n",
    "    #for the second convolution layer:\n",
    "    W2 = tf.Variable(tf.random_normal([c2, c2, conv1_size, conv2_size]))\n",
    "    b2 = tf.Variable(tf.random_normal([conv2_size]))\n",
    "    #for the fully connected layer:\n",
    "    W3 = tf.Variable(tf.random_normal([1*1*conv2_size, full_conn_size]))\n",
    "    b3 = tf.Variable(tf.random_normal([full_conn_size]))\n",
    "    #for the output:\n",
    "    WO = tf.Variable(tf.random_normal([full_conn_size, num_classes]))\n",
    "    bO = tf.Variable(tf.random_normal([num_classes]))\n",
    "    \n",
    "    \n",
    "    #define the model structure\n",
    "    # c x c convolution, 1 input, conv1_size outputs\n",
    "    convolution = conv2d(X_prime, W1, b1, stride)\n",
    "    # then apply pooling\n",
    "    pool = maxpool2d(convolution)\n",
    "    \n",
    "    #one more convolution+pooling layer:\n",
    "    convolution2 = conv2d(pool, W2, b2, stride_2)\n",
    "    # then apply pooling\n",
    "    pool2 = maxpool2d(convolution2)\n",
    "    \n",
    "    \n",
    "    #add a fully connected layer\n",
    "    # Reshape convolution output to fit the fully connected layer input\n",
    "    full_conn = tf.reshape(pool2, [-1, 1*1*conv2_size])\n",
    "    full_conn = tf.add(tf.matmul(full_conn, W3), b3)\n",
    "    full_conn = tf.nn.relu(full_conn)\n",
    "    \n",
    "\n",
    "    # Output, class prediction\n",
    "    predictions = tf.add(tf.matmul(full_conn, WO), bO)\n",
    "    \n",
    "    # Define loss and optimizer\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=predictions, labels=y))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    \n",
    "    # Evaluate model\n",
    "    # https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3_NeuralNetworks/convolutional_network.ipynb\n",
    "    correct_pred = tf.equal(tf.argmax(predictions, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    \n",
    "    \n",
    "    # Feed Dict and RUN SESSION\n",
    "    num_batches = int(len(trainx)/batch_size)\n",
    "\n",
    "    sess = tf.Session()\n",
    "    tf.global_variables_initializer().run(session=sess)\n",
    "    #Feed the batches one by one\n",
    "    for t in range(num_epochs):\n",
    "        for i in range(num_batches):\n",
    "            batch_X, batch_y = next_batch(trainx,trainy,[x for x in range(len(trainx))][i*batch_size:(i+1)*batch_size])\n",
    "            sess.run(optimizer, feed_dict={X: batch_X, y: batch_y})\n",
    "\n",
    "\n",
    "#    for _ in range(1000):\n",
    "#        batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "#        sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test accuracy is  0.944500003159\n"
     ]
    }
   ],
   "source": [
    "#Evaluating the algorithm. Note: This is not elegant since I couldn't find a way to introduce a tensor of size depending on other tensor in the Graph above\n",
    "\n",
    "num_test_batches = int(len(testx)/batch_size)\n",
    "accuracy_ = np.zeros(num_test_batches)\n",
    "\n",
    "for i in range(num_test_batches):\n",
    "        batch_X, batch_y = next_batch(testx,testy,[x for x in range(len(testx))][i*batch_size:(i+1)*batch_size])\n",
    "        accuracy_[i] = sess.run(accuracy, feed_dict={X: batch_X, y: batch_y})\n",
    "        \n",
    "print(\"The test accuracy is \",sum(accuracy_)/num_test_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I should have developed some tools to output the train accuracy at each epoch, but I'll call it a day"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
