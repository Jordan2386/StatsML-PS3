{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainx = pd.read_csv(\"usps_trainx.data\", header=None, delimiter=r\"\\s+\")\n",
    "trainy = pd.read_csv(\"usps_trainy.data\", header=None, delimiter=r\"\\s+\")\n",
    "testx = pd.read_csv(\"usps_testx.data\", header=None, delimiter=r\"\\s+\")\n",
    "testy = pd.read_csv(\"usps_testy.data\", header=None, delimiter=r\"\\s+\")\n",
    "\n",
    "im_height = 16 #image height\n",
    "im_width = 16 #image width\n",
    "num_classes = 10 #number of classes for classificaiton\n",
    "\n",
    "#normalise the images\n",
    "trainx = trainx / 256\n",
    "testx  = testx  / 256\n",
    "\n",
    "#transform pandas to numpy arrays\n",
    "trainx = trainx.as_matrix()\n",
    "trainy = trainy.as_matrix()\n",
    "testx = testx.as_matrix()\n",
    "testy = testy.as_matrix()\n",
    "\n",
    "#remove the second dimensions:\n",
    "trainy = trainy[:,0]\n",
    "testy = testy[:,0]\n",
    "\n",
    "#randomly shuffle the train data\n",
    "import random\n",
    "random_idx = random.sample([x for x in range(len(trainx))],len(trainx))\n",
    "trainx = trainx[random_idx,]\n",
    "trainy = trainy[random_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Transform the images in a 16x16 form; the resulting tensor would be 2000x16x16 both for training and for test data.\n",
    "# Simplified by Dr Seth Flaxman\n",
    "trainx = trainx.reshape(len(trainx),16,16)\n",
    "testx = testx.reshape(len(trainx),16,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1-hot encode the labels trainy and testy\n",
    "# Simplified by Dr Seth Flaxman\n",
    "trainy = pd.get_dummies(trainy).as_matrix()\n",
    "testy = pd.get_dummies(testy).as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some notation for convenience\n",
    "train_size = len(trainx)\n",
    "test_size = len(testx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract batches funciton that returns a dictionary ready to be fed to the model\n",
    "\n",
    "def next_batch(data_x,data_y,indices):\n",
    "    return data_x[indices,:,:] , data_y[indices,:]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining the convolution operation (with relu activation), and the maxpool operation\n",
    "# https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3_NeuralNetworks/convolutional_network.ipynb\n",
    "\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    # MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n",
    "                          padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"   \\n    # Feed Dict and RUN SESSION\\n    num_batches = int(len(trainx)/batch_size)\\n\\n    sess = tf.Session()\\n    tf.global_variables_initializer().run(session=sess)\\n    # Feed the batches one by one\\n    for t in range(num_epochs):\\n        for i in range(num_batches):\\n            batch_X, batch_y = next_batch(trainx,trainy,[x for x in range(len(trainx))][i*batch_size:(i+1)*batch_size])\\n            sess.run(optimizer, feed_dict={X: batch_X, y: batch_y})\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CONSTRUCT THE COMPUTATIONAL GRAPH\n",
    "\n",
    "\n",
    "# Introduce some hyperparameters\n",
    "\n",
    "batch_size = 5\n",
    "# an epoch is a single run over all the training data\n",
    "num_epochs = 50 #optimal: 400\n",
    "learning_rate = 0.001\n",
    "# applying dropout makes sure that the model doesn't rely on any specific parameter, and is thus more robust and prevents overfitting\n",
    "dropout = 0.98 # probability to keep a parameter\n",
    "# l2 - regularisation parameter\n",
    "alpha = 0.05\n",
    "\n",
    "c = 5 #convolution window size\n",
    "stride = 2 #stride of the convolution #DON'T CHANGE\n",
    "conv1_size = 64 #number of outputs from the conv layer\n",
    "\n",
    "c2 = 5 #2nd convolution window size\n",
    "stride_2 = 2 #stride of the 2nd convolution #DON'T CHANGE\n",
    "conv2_size = 32 #number of outputs from the 2nd conv layer\n",
    "\n",
    "full_conn_size = 1024 #number of nodes in the fully connected layer\n",
    "\n",
    "\n",
    "# Construct the graph\n",
    "\n",
    "compute_graph = tf.Graph()\n",
    "with compute_graph.as_default():\n",
    "    \n",
    "    #introduce placeholders for the data(images and labels) to later feed batches into\n",
    "    X = tf.placeholder(tf.float32, shape=(batch_size,im_height,im_width)) #images\n",
    "    y = tf.placeholder(tf.int32, shape=(batch_size,num_classes)) #labels\n",
    "\n",
    "    \n",
    "    #transform input into a 4d tensor to include the colour channels (TensorFlow demands that)\n",
    "    X_prime = tf.reshape(X, shape=[-1, im_height, im_width, 1])\n",
    "    \n",
    "    #define the parameters(variables) of the model\n",
    "    #for the convolution layer:\n",
    "    W1 = tf.Variable(tf.random_normal([c, c, 1, conv1_size]))\n",
    "    b1 = tf.Variable(tf.random_normal([conv1_size]))\n",
    "    #for the second convolution layer:\n",
    "    W2 = tf.Variable(tf.random_normal([c2, c2, conv1_size, conv2_size]))\n",
    "    b2 = tf.Variable(tf.random_normal([conv2_size]))\n",
    "    #for the fully connected layer:\n",
    "    W3 = tf.Variable(tf.random_normal([1*1*conv2_size, full_conn_size]))\n",
    "    b3 = tf.Variable(tf.random_normal([full_conn_size]))\n",
    "    #for the output:\n",
    "    WO = tf.Variable(tf.random_normal([full_conn_size, num_classes]))\n",
    "    bO = tf.Variable(tf.random_normal([num_classes]))\n",
    "    \n",
    "    \n",
    "    #define the model structure\n",
    "    # c x c convolution, 1 input, conv1_size outputs\n",
    "    convolution = conv2d(X_prime, W1, b1, stride)\n",
    "    # then apply pooling\n",
    "    pool = maxpool2d(convolution)\n",
    "    \n",
    "    #one more convolution+pooling layer:\n",
    "    convolution2 = conv2d(pool, W2, b2, stride_2)\n",
    "    # then apply pooling\n",
    "    pool2 = maxpool2d(convolution2)\n",
    "    \n",
    "    \n",
    "    #add a fully connected layer\n",
    "    # Reshape convolution output to fit the fully connected layer input\n",
    "    full_conn = tf.reshape(pool2, [-1, 1*1*conv2_size])\n",
    "    full_conn = tf.add(tf.matmul(full_conn, W3), b3)\n",
    "    full_conn = tf.nn.relu(full_conn)\n",
    "    # The dropout is applied (almost) at the end of the neural network\n",
    "    full_conn = tf.nn.dropout(full_conn, dropout)\n",
    "\n",
    "    # Output, class prediction\n",
    "    predictions = tf.add(tf.matmul(full_conn, WO), bO)\n",
    "    \n",
    "    # Define loss and optimizer\n",
    "    # To the cost add the l2_loss of all the weights (but not the biases)\n",
    "    cost = (tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=predictions, labels=y)) + \\\n",
    "            alpha * (tf.nn.l2_loss(WO) + tf.nn.l2_loss(W1) + tf.nn.l2_loss(W2) + tf.nn.l2_loss(W3)))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    \n",
    "    # Evaluate model\n",
    "    # https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3_NeuralNetworks/convolutional_network.ipynb\n",
    "    correct_pred = tf.equal(tf.argmax(predictions, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    \n",
    "\"\"\"\"   \n",
    "    # Feed Dict and RUN SESSION\n",
    "    num_batches = int(len(trainx)/batch_size)\n",
    "\n",
    "    sess = tf.Session()\n",
    "    tf.global_variables_initializer().run(session=sess)\n",
    "    # Feed the batches one by one\n",
    "    for t in range(num_epochs):\n",
    "        for i in range(num_batches):\n",
    "            batch_X, batch_y = next_batch(trainx,trainy,[x for x in range(len(trainx))][i*batch_size:(i+1)*batch_size])\n",
    "            sess.run(optimizer, feed_dict={X: batch_X, y: batch_y})\n",
    "\"\"\"\n",
    "\n",
    "#    for _ in range(1000):\n",
    "#        batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "#        sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-31112ed62a81>:21 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Train accuracy at epoch 1 is: 0.361666675078\n",
      "Valid accuracy at epoch 1 is: 0.495000012591\n",
      "Train accuracy at epoch 2 is: 0.611666679341\n",
      "Valid accuracy at epoch 2 is: 0.600000014901\n",
      "Train accuracy at epoch 3 is: 0.70444445697\n",
      "Valid accuracy at epoch 3 is: 0.675000015646\n",
      "Train accuracy at epoch 4 is: 0.774444455074\n",
      "Valid accuracy at epoch 4 is: 0.715000013262\n",
      "Train accuracy at epoch 5 is: 0.824444453915\n",
      "Valid accuracy at epoch 5 is: 0.760000012815\n",
      "Train accuracy at epoch 6 is: 0.849444452466\n",
      "Valid accuracy at epoch 6 is: 0.745000012219\n",
      "Train accuracy at epoch 7 is: 0.883333339956\n",
      "Valid accuracy at epoch 7 is: 0.745000014082\n",
      "Train accuracy at epoch 8 is: 0.902777783159\n",
      "Valid accuracy at epoch 8 is: 0.795000010729\n",
      "Train accuracy at epoch 9 is: 0.911111116244\n",
      "Valid accuracy at epoch 9 is: 0.845000008494\n",
      "Train accuracy at epoch 10 is: 0.924444448782\n",
      "Valid accuracy at epoch 10 is: 0.820000010729\n",
      "Train accuracy at epoch 15 is: 0.950555558503\n",
      "Valid accuracy at epoch 15 is: 0.840000007302\n",
      "Train accuracy at epoch 20 is: 0.9727777794\n",
      "Valid accuracy at epoch 20 is: 0.865000008047\n",
      "Train accuracy at epoch 25 is: 0.978888890147\n",
      "Valid accuracy at epoch 25 is: 0.885000006109\n",
      "Train accuracy at epoch 30 is: 0.982777778804\n",
      "Valid accuracy at epoch 30 is: 0.920000004768\n",
      "Train accuracy at epoch 35 is: 0.985000000894\n",
      "Valid accuracy at epoch 35 is: 0.870000007004\n",
      "Train accuracy at epoch 40 is: 0.982777778804\n",
      "Valid accuracy at epoch 40 is: 0.895000006258\n",
      "Train accuracy at epoch 45 is: 0.992777778208\n",
      "Valid accuracy at epoch 45 is: 0.92500000447\n",
      "Train accuracy at epoch 50 is: 0.992777778208\n",
      "Valid accuracy at epoch 50 is: 0.930000004172\n",
      "Train accuracy at epoch 55 is: 0.995000000298\n",
      "Valid accuracy at epoch 55 is: 0.92500000447\n",
      "Train accuracy at epoch 60 is: 0.994444444776\n",
      "Valid accuracy at epoch 60 is: 0.895000006258\n",
      "Train accuracy at epoch 65 is: 0.990555556118\n",
      "Valid accuracy at epoch 65 is: 0.935000003874\n",
      "Train accuracy at epoch 70 is: 0.989444445074\n",
      "Valid accuracy at epoch 70 is: 0.935000003874\n",
      "Train accuracy at epoch 75 is: 0.985555556417\n",
      "Valid accuracy at epoch 75 is: 0.940000003576\n",
      "Train accuracy at epoch 80 is: 0.981666667759\n",
      "Valid accuracy at epoch 80 is: 0.930000004172\n",
      "Train accuracy at epoch 85 is: 0.980555556715\n",
      "Valid accuracy at epoch 85 is: 0.890000006557\n",
      "Train accuracy at epoch 90 is: 0.974444445968\n",
      "Valid accuracy at epoch 90 is: 0.920000004768\n",
      "Train accuracy at epoch 95 is: 0.973888890445\n",
      "Valid accuracy at epoch 95 is: 0.910000005364\n",
      "Train accuracy at epoch 100 is: 0.955000002599\n",
      "Valid accuracy at epoch 100 is: 0.955000002682\n",
      "Train accuracy at epoch 150 is: 0.921666671253\n",
      "Valid accuracy at epoch 150 is: 0.92500000447\n",
      "Train accuracy at epoch 200 is: 0.928333337522\n",
      "Valid accuracy at epoch 200 is: 0.905000004917\n",
      "Train accuracy at epoch 250 is: 0.928333337439\n",
      "Valid accuracy at epoch 250 is: 0.920000004023\n",
      "Train accuracy at epoch 300 is: 0.930000004007\n",
      "Valid accuracy at epoch 300 is: 0.915000004321\n",
      "Train accuracy at epoch 350 is: 0.931111115134\n",
      "Valid accuracy at epoch 350 is: 0.910000005364\n",
      "Train accuracy at epoch 400 is: 0.929444448484\n",
      "Valid accuracy at epoch 400 is: 0.930000003427\n"
     ]
    }
   ],
   "source": [
    "# TUNE HYPERPARAMETERS\n",
    "# Note: this can take a while to run\n",
    "# Extract (from the test set) a small validation set to test the model accuracy while tuning the hyperparameters\n",
    "\n",
    "valid_size = 200\n",
    "\n",
    "trainx_ = trainx[0:train_size-valid_size]\n",
    "trainy_ = trainy[0:train_size-valid_size]\n",
    "validx = trainx[train_size-valid_size:]\n",
    "validy = trainy[train_size-valid_size:]\n",
    "\n",
    "num_train_batches = int(len(trainx_)/batch_size)\n",
    "train_accuracy = np.zeros(num_train_batches)\n",
    "\n",
    "num_valid_batches = int(len(validx)/batch_size)\n",
    "valid_accuracy = np.zeros(num_valid_batches)\n",
    "\n",
    "\n",
    "# Initialise all variables and run the session:\n",
    "with tf.Session(graph=compute_graph) as sess:\n",
    "    tf.initialize_all_variables().run(session=sess)\n",
    "    \n",
    "    # Feed the batches one by one\n",
    "    for t in range(num_epochs):\n",
    "        for i in range(num_train_batches):\n",
    "            batch_X, batch_y = next_batch(trainx_,trainy_,[x for x in range(train_size-valid_size)][i*batch_size:(i+1)*batch_size])\n",
    "            sess.run(optimizer, feed_dict={X: batch_X, y: batch_y})\n",
    "            train_accuracy[i] = sess.run(accuracy, feed_dict={X: batch_X, y: batch_y})\n",
    "        \n",
    "        # report train/valid accuracy regularly in order to perform early stoping\n",
    "        if((t+1) % 20 == 0 or ((t+1) % 5 == 0 and (t+1) <= 100) or (t+1) <= 10):\n",
    "            print(\"Train accuracy at epoch\", t+1, \"is:\", sum(train_accuracy)/num_train_batches)\n",
    "            # validation accuracy\n",
    "            for i in range(num_valid_batches):\n",
    "                batch_X, batch_y = next_batch(validx,validy,[x for x in range(valid_size)][i*batch_size:(i+1)*batch_size])\n",
    "                valid_accuracy[i] = sess.run(accuracy, feed_dict={X: batch_X, y: batch_y})\n",
    "            print(\"Valid accuracy at epoch\", t+1, \"is:\", sum(valid_accuracy)/num_valid_batches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TUNE HYPERPARAMETERS\n",
    "# Do cross-validation to check the model performance and tune the parameters (num layers, layer types, batch size, etc.)\n",
    "# Note: it's not wise to use the test set in hyperparameter tuning\n",
    "# Note: cross-validation is much slower than fixed-set validation\n",
    "\n",
    "#num_cross_valid = 10 #number of cross-validation sectors\n",
    "#train_accuracy = np.zeros(num_cross_valid)\n",
    "\n",
    "# Train the model on each 9 of the 10 train data sectors, while recording the test accuracy on the remaining segment\n",
    "#for i in range(num_cross_valid):\n",
    "#    train_batch_X, train_batch_y = next_batch(trainx,trainy,[x for x in range(len(trainx))][i*batch_size:(i+1)*batch_size])\n",
    "#    sess.run(optimizer, feed_dict={X: train_batch_X, y: train_batch_y})\n",
    "#    test_batch_X, test_batch_y = next_batch(trainx,trainy,[x for x in range(len(trainx))][i*batch_size:(i+1)*batch_size])\n",
    "#    train_accuracy[i] = sess.run(accuracy, feed_dict={X: batch_X, y: batch_y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-15-267ffd6154d4>:12 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "The train accuracy at epoch 1 is: 0.3570000083\n",
      "The train accuracy at epoch 2 is: 0.629500012323\n",
      "The train accuracy at epoch 3 is: 0.723500012234\n",
      "The train accuracy at epoch 4 is: 0.800500010028\n",
      "The train accuracy at epoch 5 is: 0.83200000871\n",
      "The train accuracy at epoch 6 is: 0.858000007793\n",
      "The train accuracy at epoch 7 is: 0.893000005931\n",
      "The train accuracy at epoch 8 is: 0.902500005811\n",
      "The train accuracy at epoch 9 is: 0.913500004932\n",
      "The train accuracy at epoch 10 is: 0.934500003904\n",
      "The train accuracy at epoch 15 is: 0.961500002295\n",
      "The train accuracy at epoch 20 is: 0.97550000146\n",
      "The train accuracy at epoch 25 is: 0.980000001192\n",
      "The train accuracy at epoch 30 is: 0.985000000894\n",
      "The train accuracy at epoch 35 is: 0.989000000656\n",
      "The train accuracy at epoch 40 is: 0.985500000864\n",
      "The train accuracy at epoch 45 is: 0.989500000626\n",
      "The train accuracy at epoch 50 is: 0.992500000447\n",
      "The test accuracy is  0.928000003994\n"
     ]
    }
   ],
   "source": [
    "# NOW TRAIN THE MODEL OVER THE WHOLE DATASET AND REPORT TEST ACCURACY\n",
    "# Note: this can take a while to run\n",
    "\n",
    "num_batches = int(train_size/batch_size)\n",
    "train_accuracy = np.zeros(num_batches)\n",
    "\n",
    "num_test_batches = int(test_size/batch_size)\n",
    "accuracy_ = np.zeros(num_test_batches)\n",
    "\n",
    "# Initialise all variables and run the session\n",
    "with tf.Session(graph=compute_graph) as sess:\n",
    "    tf.initialize_all_variables().run(session=sess)\n",
    "\n",
    "    # Feed the batches one by one\n",
    "    for t in range(num_epochs):\n",
    "        for i in range(num_batches):\n",
    "            batch_X, batch_y = next_batch(trainx,trainy,[x for x in range(train_size)][i*batch_size:(i+1)*batch_size])\n",
    "            sess.run(optimizer, feed_dict={X: batch_X, y: batch_y})\n",
    "            train_accuracy[i] = sess.run(accuracy, feed_dict={X: batch_X, y: batch_y})\n",
    "        if((t+1) % 20 == 0 or ((t+1) % 5 == 0 and (t+1) <= 100) or (t+1) <= 10):\n",
    "            print(\"The train accuracy at epoch\", t+1, \"is:\", sum(train_accuracy)/num_batches)\n",
    "            \n",
    "    \n",
    "        \n",
    "    \n",
    "    for i in range(num_test_batches):\n",
    "        batch_X, batch_y = next_batch(testx,testy,[x for x in range(len(testx))][i*batch_size:(i+1)*batch_size])\n",
    "        accuracy_[i] = sess.run(accuracy, feed_dict={X: batch_X, y: batch_y})\n",
    "        \n",
    "    print(\"The test accuracy is \",sum(accuracy_)/num_test_batches)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
